{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tanmoypaul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tanmoypaul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "non_clickbait_url = \"http://www.cs.columbia.edu/~sarahita/CL/non_clickbait_data.txt\"\n",
    "clickbait_url = \"http://www.cs.columbia.edu/~sarahita/CL/clickbait_data.txt\"\n",
    "\n",
    "# read url .txt file into string \"data\"\n",
    "def get_data(url):\n",
    "  data = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "  return data\n",
    "\n",
    "non_clickbait_data = get_data(non_clickbait_url)\n",
    "clickbait_data = get_data(clickbait_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine clickbait and non-clickbait data in a single list\n",
    "non_clickbait_headlines = non_clickbait_data.rstrip('\\n').split('\\n')\n",
    "clickbait_headlines = clickbait_data.rstrip('\\n').split('\\n')\n",
    "all_headlines = non_clickbait_headlines + clickbait_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of corresponding labels\n",
    "non_cb_labels = [0] * len(non_clickbait_headlines)\n",
    "cb_labels = [1] * len(clickbait_headlines)\n",
    "all_labels = non_cb_labels + cb_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features: bag of stop words\n",
    "def stop_words(texts):\n",
    "        bow = []        # what we are returning\n",
    "        eng_stopwords = stopwords.words('english')      # all english stopwords\n",
    "        for text in texts:      # for headline in headlines\n",
    "                counts = []     # list of counts\n",
    "                tokens = nltk.word_tokenize(text.lower())  # tokens is dict of tokenized headline\n",
    "                for sw in eng_stopwords:                # go through stopwords\n",
    "                        sw_count = tokens.count(sw)\n",
    "                        counts.append(sw_count)         # COUNT OF ALL STOPWORD APPEARANCES IN ONE HEADLINE\n",
    "                bow.append(counts)                      # 2D ARRAY OF SW COUNT FOR EACH HEADLINE\n",
    "        bow_np = np.array(bow).astype(float)\n",
    "        return bow_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "stop_words_features = stop_words(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31998, 179)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8735535323538606\n",
      "[0.88       0.8790625  0.866875   0.878125   0.8815625  0.8821875\n",
      " 0.8834375  0.87125    0.8380744  0.87496093]\n"
     ]
    }
   ],
   "source": [
    "# convert features and labels to numpy arrays\n",
    "X = stop_words_features\n",
    "Y = np.array(all_labels)\n",
    "\n",
    "# run classifier using 10-fold cross validation\n",
    "# report mean accuracy \n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
    "print(scores.mean())\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Syntactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts = {}\n",
    "\n",
    "for headline in all_headlines:\n",
    "    pos_headline = pos_tag(word_tokenize(headline))\n",
    "    for token, tag in pos_headline:\n",
    "        if tag in pos_counts:\n",
    "            pos_counts[tag] += 1\n",
    "        else: \n",
    "            pos_counts[tag] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of pos_counts: 43\n",
      "Ordered: [('VBZ', 7548), ('VB', 7798), ('PRP', 8294), ('CD', 11959), ('JJ', 12661), ('DT', 12876), ('NNS', 15322), ('NN', 22577), ('IN', 29113), ('NNP', 117851)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of pos_counts: {len(pos_counts)}')\n",
    "pos_top_ten = list(sorted(pos_counts.items(), key=lambda item: item[1]))[-10:]\n",
    "print(f'Ordered: {pos_top_ten}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features: bag of pos tags\n",
    "def pos_accuracy(texts, pos_top_ten):\n",
    "        bow = []        # what we are returning\n",
    "        for text in texts:      # for headline in headlines\n",
    "                counts = []     # list of counts\n",
    "                tokens = nltk.word_tokenize(text)  # tokens is dict of tokenized headline\n",
    "                tags = pos_tag(tokens)\n",
    "                tags_list = [item for t in tags for item in t]\n",
    "                for pos in pos_top_ten:               \n",
    "                        pos_count = tags_list.count(pos)\n",
    "                        counts.append(pos_count)        \n",
    "                bow.append(counts)                     \n",
    "        bow_np = np.array(bow).astype(float)\n",
    "        return bow_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags = pos_tag(nltk.word_tokenize(all_headlines[0]))\n",
    "# tags_list = [item for t in tags for item in t]\n",
    "# print(tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only pos list: ['VBZ', 'VB', 'PRP', 'CD', 'JJ', 'DT', 'NNS', 'NN', 'IN', 'NNP']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31998, 10)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_top_ten = [tag for tag, count in pos_top_ten]\n",
    "print(f'only pos list: {pos_top_ten}')\n",
    "pos_features = pos_accuracy(all_headlines, pos_top_ten)\n",
    "pos_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7732356888871521\n",
      "[0.7765625  0.78       0.761875   0.771875   0.7784375  0.7853125\n",
      " 0.77       0.76625    0.77305408 0.76899031]\n"
     ]
    }
   ],
   "source": [
    "# convert features and labels to numpy arrays\n",
    "X = pos_features\n",
    "Y = np.array(all_labels)\n",
    "\n",
    "# run classifier using 10-fold cross validation\n",
    "# report mean accuracy \n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
    "print(scores.mean())\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "n = 1\n",
    "unigram_count = {}\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "for headline in all_headlines:\n",
    "    unigrams = list(ngrams(headline.split(), n))\n",
    "\n",
    "    for item in list(unigrams):\n",
    "        for word in item:\n",
    "            if word.lower() not in eng_stopwords:\n",
    "                if word not in unigram_count:\n",
    "                    unigram_count[word] = 1\n",
    "                else:\n",
    "                    unigram_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of unigram_count: 35578\n",
      "Ordered: [('Ever', 326), ('Every', 333), ('dies', 336), ('Time', 339), ('Get', 343), ('World', 344), ('First', 363), ('18', 364), ('15', 373), ('23', 377), ('One', 391), ('Life', 402), ('Need', 414), (\"Here's\", 423), ('Best', 477), ('2015', 494), ('Like', 503), ('U.S.', 537), ('Times', 550), ('Actually', 579), ('19', 593), ('US', 599), ('Based', 607), ('21', 663), ('17', 687), ('Make', 789), ('Know', 804), ('People', 920), ('Things', 996), ('New', 1036)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of unigram_count: {len(unigram_count)}')\n",
    "unigram_top_thirty = list(sorted(unigram_count.items(), key=lambda item : item[1]))[-30:]\n",
    "print(f'Ordered: {unigram_top_thirty}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features: bag of unigrams\n",
    "def unigram_accuracy(texts, unigram_top_thirty):\n",
    "        bow = []        # what we are returning\n",
    "        for text in texts:      # for headline in headlines\n",
    "                counts = []     # list of counts\n",
    "                tokens = nltk.word_tokenize(text)  # tokens is dict of tokenized headline\n",
    "                for unigram in unigram_top_thirty:               \n",
    "                        unigram_count = tokens.count(unigram)\n",
    "                        counts.append(unigram_count)        \n",
    "                bow.append(counts)                     \n",
    "        bow_np = np.array(bow).astype(float)\n",
    "        return bow_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only unigram list: ['Ever', 'Every', 'dies', 'Time', 'Get', 'World', 'First', '18', '15', '23', 'One', 'Life', 'Need', \"Here's\", 'Best', '2015', 'Like', 'U.S.', 'Times', 'Actually', '19', 'US', 'Based', '21', '17', 'Make', 'Know', 'People', 'Things', 'New']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31998, 30)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_top_thirty = [tag for tag, count in unigram_top_thirty]\n",
    "print(f'Only unigram list: {unigram_top_thirty}')\n",
    "unigram_features = unigram_accuracy(all_headlines, unigram_top_thirty)\n",
    "unigram_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7173213211159737\n",
      "[0.739375   0.7359375  0.7346875  0.7365625  0.7428125  0.7409375\n",
      " 0.7334375  0.7271875  0.54517037 0.73710535]\n"
     ]
    }
   ],
   "source": [
    "# convert features and labels to numpy arrays\n",
    "X = unigram_features\n",
    "Y = np.array(all_labels)\n",
    "\n",
    "# run classifier using 10-fold cross validation\n",
    "# report mean accuracy \n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
    "print(scores.mean())\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_counts = {}\n",
    "\n",
    "for headline in all_headlines:\n",
    "    tokenized_headline = word_tokenize(headline)\n",
    "    for token in tokenized_headline:\n",
    "        if token not in string.punctuation:\n",
    "            continue\n",
    "        if token in punctuation_counts:\n",
    "            punctuation_counts[token] += 1\n",
    "        else: \n",
    "            punctuation_counts[token] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of punctuation_counts: 23\n",
      "Ordered: [('|', 1), ('/', 1), ('[', 1), (']', 1), ('`', 2), ('=', 3), ('+', 4), ('@', 6), ('*', 30), ('-', 33), ('!', 40), ('#', 57), ('(', 91), (')', 91), ('&', 99), ('%', 144), ('?', 162), (';', 230), ('$', 259), (\"'\", 602), ('.', 630), (':', 1073), (',', 4081)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of punctuation_counts: {len(punctuation_counts)}')\n",
    "punctuation_list = list(sorted(punctuation_counts.items(), key=lambda item: item[1]))\n",
    "print(f'Ordered: {punctuation_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features: bag of punctuation\n",
    "def punctuation_accuracy(texts, punctuation_list):\n",
    "        bow = []        # what we are returning\n",
    "        for text in texts:      # for headline in headlines\n",
    "                counts = []     # list of counts\n",
    "                tokens = nltk.word_tokenize(text)  # tokens is dict of tokenized headline\n",
    "                for punctuation in punctuation_list:               \n",
    "                        punctuation_count = tokens.count(punctuation)\n",
    "                        counts.append(punctuation_count)        \n",
    "                bow.append(counts)                     \n",
    "        bow_np = np.array(bow).astype(float)\n",
    "        return bow_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only punctuation list: ['|', '/', '[', ']', '`', '=', '+', '@', '*', '-', '!', '#', '(', ')', '&', '%', '?', ';', '$', \"'\", '.', ':', ',']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31998, 23)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_list = [tag for tag, count in punctuation_list]\n",
    "print(f'Only punctuation list: {punctuation_list}')\n",
    "punctuation_features = punctuation_accuracy(all_headlines, punctuation_list)\n",
    "punctuation_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5012212800875273\n",
      "[0.494375  0.4859375 0.4946875 0.49      0.495625  0.4871875 0.49375\n",
      " 0.4896875 0.5948734 0.4860894]\n"
     ]
    }
   ],
   "source": [
    "# convert features and labels to numpy arrays\n",
    "X = punctuation_features\n",
    "Y = np.array(all_labels)\n",
    "\n",
    "# run classifier using 10-fold cross validation\n",
    "# report mean accuracy \n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
    "print(scores.mean())\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = 0\n",
    "total_chars = 0\n",
    "unique_words = []\n",
    "long_words = 0\n",
    "\n",
    "for headline in all_headlines:\n",
    "    tokenized_headline = word_tokenize(headline)\n",
    "    for token in tokenized_headline:\n",
    "        token = token.lower()\n",
    "        if token not in string.punctuation:\n",
    "            total_words += 1\n",
    "            total_chars += len(token)\n",
    "            if len(token) >= 6:\n",
    "                long_words += 1\n",
    "            if token not in unique_words:\n",
    "                unique_words.append(token)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average number of characters per word    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461156\n",
      "Average characters: 4.867453504292296\n"
     ]
    }
   ],
   "source": [
    "print(total_chars)\n",
    "avg_chars_per_word = total_chars / total_words\n",
    "print(f'Average characters: {avg_chars_per_word}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #unique words/#total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24854\n",
      "Average words: 0.08279450612780614\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_words))\n",
    "unique_to_total = len(unique_words) / total_words\n",
    "print(f'Average words: {unique_to_total}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300189\n"
     ]
    }
   ],
   "source": [
    "print(total_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of “long” words - words with >= 6 letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104454\n"
     ]
    }
   ],
   "source": [
    "print(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only complexity list: [[4.867453504292296, 0.08279450612780614, 300189, 104454], [4.867453504292296, 0.08279450612780614, 300189, 104454], [4.867453504292296, 0.08279450612780614, 300189, 104454]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31998, 4)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexity_list = [[avg_chars_per_word, unique_to_total, total_words, long_words]] * 31998\n",
    "print(f'Only complexity list: {complexity_list[0:3]}')\n",
    "complexity_features = np.array(complexity_list).astype(float)\n",
    "complexity_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49996874023132226\n",
      "[0.5       0.5       0.5       0.5       0.5       0.5       0.5\n",
      " 0.5       0.4998437 0.4998437]\n"
     ]
    }
   ],
   "source": [
    "# convert features and labels to numpy arrays\n",
    "X = complexity_features\n",
    "Y = np.array(all_labels)\n",
    "\n",
    "# run classifier using 10-fold cross validation\n",
    "# report mean accuracy \n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
    "print(scores.mean())\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Slang words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}\n",
    "\n",
    "slang_words = []\n",
    "for acronym in abbreviations:\n",
    "    slang_words.append(acronym)\n",
    "\n",
    "slang_counts = {}\n",
    "unique_words = []\n",
    "\n",
    "for headline in all_headlines:\n",
    "    tokenized_headline = word_tokenize(headline)\n",
    "    for token in tokenized_headline:\n",
    "        token = token.lower()\n",
    "        if token in slang_words:\n",
    "            if token in slang_counts:\n",
    "                slang_counts[token] += 1\n",
    "            else: \n",
    "                slang_counts[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of slang_counts: 37\n",
      "Ordered: [('bros', 5), ('irl', 7), ('fyi', 10), ('fc', 11), ('bae', 11), ('asap', 14), ('omg', 15), ('ie', 26), ('wtf', 30), ('diy', 31)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of slang_counts: {len(slang_counts)}')\n",
    "slang_top_ten = list(sorted(slang_counts.items(), key=lambda item: item[1]))[-10:]\n",
    "print(f'Ordered: {slang_top_ten}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features: bag of pos tags\n",
    "def slang_accuracy(texts, slang_top_ten):\n",
    "        bow = []        # what we are returning\n",
    "        for text in texts:      # for headline in headlines\n",
    "                counts = []     # list of counts\n",
    "                tokens = nltk.word_tokenize(text)  # tokens is dict of tokenized headline\n",
    "                for slang in slang_top_ten:               \n",
    "                        slang_count = tokens.count(slang)\n",
    "                        counts.append(slang_count)        \n",
    "                bow.append(counts)                     \n",
    "        bow_np = np.array(bow).astype(float)\n",
    "        return bow_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only slang list: ['bros', 'irl', 'fyi', 'fc', 'bae', 'asap', 'omg', 'ie', 'wtf', 'diy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31998, 10)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang_top_ten = [tag for tag, count in slang_top_ten]\n",
    "print(f'Only slang list: {slang_top_ten}')\n",
    "slang_features = slang_accuracy(all_headlines, slang_top_ten)\n",
    "slang_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5007500195373554\n",
      "[0.500625   0.5009375  0.501875   0.500625   0.500625   0.5009375\n",
      " 0.500625   0.500625   0.4998437  0.50078149]\n"
     ]
    }
   ],
   "source": [
    "# convert features and labels to numpy arrays\n",
    "X = slang_features\n",
    "Y = np.array(all_labels)\n",
    "\n",
    "# run classifier using 10-fold cross validation\n",
    "# report mean accuracy \n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
    "print(scores.mean())\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31998, 256)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features = np.concatenate((stop_words_features, pos_features, unigram_features, punctuation_features, complexity_features, slang_features), axis=1)\n",
    "combined_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9272457115504846\n",
      "[0.92875    0.9296875  0.916875   0.9296875  0.929375   0.93125\n",
      " 0.923125   0.9209375  0.93091591 0.9318537 ]\n"
     ]
    }
   ],
   "source": [
    "# convert features and labels to numpy arrays\n",
    "X = combined_features\n",
    "Y = np.array(all_labels)\n",
    "\n",
    "# run classifier using 10-fold cross validation\n",
    "# report mean accuracy \n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
    "print(scores.mean())\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: difference of punctuation use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of nonclickbait_punctuation_counts: 17\n",
      "Ordered: [('+', 1), ('=', 1), ('*', 1), ('`', 2), ('!', 8), ('-', 29), ('(', 32), (')', 32), ('&', 50), ('%', 117), ('?', 119), ('$', 202), (';', 228), (\"'\", 472), ('.', 547), (':', 711), (',', 3356)]\n"
     ]
    }
   ],
   "source": [
    "nonclickbait_punctuation_counts = {}\n",
    "\n",
    "for headline in non_clickbait_headlines:\n",
    "    tokenized_headline = word_tokenize(headline)\n",
    "    for token in tokenized_headline:\n",
    "        if token not in string.punctuation:\n",
    "            continue\n",
    "        if token in nonclickbait_punctuation_counts:\n",
    "            nonclickbait_punctuation_counts[token] += 1\n",
    "        else: \n",
    "            nonclickbait_punctuation_counts[token] = 1\n",
    "\n",
    "print(f'Length of nonclickbait_punctuation_counts: {len(nonclickbait_punctuation_counts)}')\n",
    "nonclickbait_punctuation_list = list(sorted(nonclickbait_punctuation_counts.items(), key=lambda item: item[1]))\n",
    "print(f'Ordered: {nonclickbait_punctuation_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of clickbait_punctuation_counts: 22\n",
      "Ordered: [('|', 1), ('/', 1), ('[', 1), (']', 1), ('=', 2), (';', 2), ('+', 3), ('-', 4), ('@', 6), ('%', 27), ('*', 29), ('!', 32), ('?', 43), ('&', 49), ('#', 57), ('$', 57), ('(', 59), (')', 59), ('.', 83), (\"'\", 130), (':', 362), (',', 725)]\n"
     ]
    }
   ],
   "source": [
    "clickbait_punctuation_counts = {}\n",
    "\n",
    "for headline in clickbait_headlines:\n",
    "    tokenized_headline = word_tokenize(headline)\n",
    "    for token in tokenized_headline:\n",
    "        if token not in string.punctuation:\n",
    "            continue\n",
    "        if token in clickbait_punctuation_counts:\n",
    "            clickbait_punctuation_counts[token] += 1\n",
    "        else: \n",
    "            clickbait_punctuation_counts[token] = 1\n",
    "\n",
    "print(f'Length of clickbait_punctuation_counts: {len(clickbait_punctuation_counts)}')\n",
    "clickbait_punctuation_list = list(sorted(clickbait_punctuation_counts.items(), key=lambda item: item[1]))\n",
    "print(f'Ordered: {clickbait_punctuation_list}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
