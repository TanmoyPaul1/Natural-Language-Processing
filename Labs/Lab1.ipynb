{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tanmoypaul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tanmoypaul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "shakespeare_url = \"http://www.cs.columbia.edu/~sarahita/CL/lab1/shakespeare.txt\"\n",
    "news_url = \"http://www.cs.columbia.edu/~sarahita/CL/lab1/news.txt\"\n",
    "swbd_url = \"http://www.cs.columbia.edu/~sarahita/CL/lab1/swbd.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read url.txt file into string \"data\"\n",
    "def get_data(url):\n",
    "    data = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    return data\n",
    "\n",
    "shakespeare_data = get_data(shakespeare_url)\n",
    "news_data = get_data(news_url)\n",
    "swbd_data = get_data(swbd_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokens: 1229288\n",
      "Length of shakespeare_vocab: 39531\n"
     ]
    }
   ],
   "source": [
    "shakespeare_token = word_tokenize(shakespeare_data)\n",
    "print(f'Length of tokens: {len(shakespeare_token)}')\n",
    "\n",
    "shakespeare_vocab = {}\n",
    "for token in shakespeare_token:\n",
    "    if token in shakespeare_vocab:\n",
    "        shakespeare_vocab[token] += 1\n",
    "    else: \n",
    "        shakespeare_vocab[token] = 1\n",
    "\n",
    "\n",
    "print(f'Length of shakespeare_vocab: {len(shakespeare_vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokens: 33379\n",
      "Length of news_vocab: 7247\n"
     ]
    }
   ],
   "source": [
    "news_tokens = word_tokenize(news_data)\n",
    "print(f'Length of tokens: {len(news_tokens)}')\n",
    "\n",
    "news_vocab = {}\n",
    "for token in news_tokens:\n",
    "    if token in news_vocab:\n",
    "        news_vocab[token] += 1\n",
    "    else: \n",
    "        news_vocab[token] = 1\n",
    "\n",
    "\n",
    "print(f'Length of news_vocab: {len(news_vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of token: 96222\n",
      "length of swbd_vocab: 5225\n"
     ]
    }
   ],
   "source": [
    "swbd_tokens = word_tokenize(swbd_data)\n",
    "print(f'Length of token: {len(swbd_tokens)}')\n",
    "\n",
    "swbd_vocab = {}\n",
    "for token in swbd_tokens:\n",
    "    if token in swbd_vocab:\n",
    "        swbd_vocab[token] += 1\n",
    "    else: \n",
    "        swbd_vocab[token] = 1\n",
    "\n",
    "\n",
    "print(f'length of swbd_vocab: {len(swbd_vocab)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered: [('a', 13838), ('’', 16736), ('of', 16983), ('to', 17285), (';', 17357), ('and', 20145), ('I', 23305), ('the', 25536), ('.', 81386), (',', 92610)]\n"
     ]
    }
   ],
   "source": [
    "shakespeare_normalized = list(sorted(shakespeare_vocab.items(), key=lambda item: item[1]))\n",
    "print(f'Ordered: {shakespeare_normalized[-10:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered: [('on', 323), (\"'s\", 431), ('in', 608), ('a', 636), ('and', 637), ('of', 658), (',', 673), ('to', 855), ('.', 1201), ('the', 1343)]\n"
     ]
    }
   ],
   "source": [
    "news_normalized = list(sorted(news_vocab.items(), key=lambda item: item[1]))\n",
    "print(f'Ordered: {news_normalized[-10:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered: [('it', 1501), ('a', 1568), ('you', 1671), ('that', 1687), ('and', 1710), ('the', 1856), ('I', 3092), ('.', 5097), (':', 5303), (',', 10323)]\n"
     ]
    }
   ],
   "source": [
    "swbd_normalized = list(sorted(swbd_vocab.items(), key=lambda item: item[1]))\n",
    "print(f'Ordered: {swbd_normalized[-10:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Shakespeare tokens: 522253\n",
      "Length of shakespeare_vocab: 32874\n",
      "Ordered: [('good', 2950), ('sir', 2979), ('king', 3107), ('lord', 3162), ('thee', 3374), (\"'s\", 3569), ('shall', 3844), (\"'d\", 4305), ('thy', 4357), ('thou', 5865)]\n",
      "0.06294650294014587\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "import string\n",
    "\n",
    "# Make all lower case\n",
    "shakespeare_token_lower_map = map(lambda x:x.lower(), shakespeare_token)\n",
    "shakespeare_token = list(shakespeare_token_lower_map)\n",
    "\n",
    "# Remove punctuation\n",
    "punctuation = set()\n",
    "for sign in string.punctuation:\n",
    "    punctuation.add(sign)\n",
    "\n",
    "punctuation.add('’')\n",
    "punctuation.add(\"''\")\n",
    "punctuation.add(\"``\")\n",
    "punctuation.add('--')\n",
    "\n",
    "\n",
    "shakespeare_dic = {}\n",
    "for token in shakespeare_token:\n",
    "    if token not in shakespeare_dic:\n",
    "        shakespeare_dic[token] = 1\n",
    "    else:\n",
    "        shakespeare_dic[token] += 1\n",
    "\n",
    "for symbol in punctuation:\n",
    "    if symbol in shakespeare_token:\n",
    "        del shakespeare_dic[symbol]\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# Remove stop words\n",
    "for words in stopWords:\n",
    "    if words in shakespeare_token:\n",
    "        del shakespeare_dic[words]\n",
    "\n",
    "\n",
    "temp = []\n",
    "for key in shakespeare_dic:\n",
    "    repeat = shakespeare_dic[key]\n",
    "    for i in range(repeat):\n",
    "        temp.append(key)\n",
    "\n",
    "for i in punctuation:\n",
    "    if i in temp:\n",
    "        print(i)\n",
    "\n",
    "shakespeare_token = temp\n",
    "\n",
    "\n",
    "shakespeare_vocab = {}\n",
    "for token in shakespeare_token:\n",
    "    if token in shakespeare_vocab:\n",
    "        shakespeare_vocab[token] += 1\n",
    "    else: \n",
    "        shakespeare_vocab[token] = 1\n",
    "\n",
    "\n",
    "\n",
    "print(f'Length of Shakespeare tokens: {len(shakespeare_token)}')\n",
    "print(f'Length of shakespeare_vocab: {len(shakespeare_vocab)}')\n",
    "x = list(sorted(shakespeare_vocab.items(), key=lambda item: item[1]))\n",
    "print(f'Ordered: {x[-10:]}')\n",
    "\n",
    "print(len(shakespeare_vocab) / len(shakespeare_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of news tokens: 18854\n",
      "Length of news_vocab: 6390\n",
      "Ordered: [('first', 61), ('students', 63), ('school', 65), (\"n't\", 65), ('president', 74), ('would', 75), ('new', 90), ('trump', 141), ('said', 193), (\"'s\", 431)]\n",
      "0.3389201230508115\n"
     ]
    }
   ],
   "source": [
    "# Make all lower case\n",
    "news_token_lower_map = map(lambda x:x.lower(), news_tokens)\n",
    "news_token = list(news_token_lower_map)\n",
    "\n",
    "\n",
    "news_dic = {}\n",
    "for token in news_token:\n",
    "    if token not in news_dic:\n",
    "        news_dic[token] = 1\n",
    "    else:\n",
    "        news_dic[token] += 1\n",
    "\n",
    "for symbol in punctuation:\n",
    "    if symbol in news_token:\n",
    "        del news_dic[symbol]\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# Remove stop words\n",
    "for words in stopWords:\n",
    "    if words in news_token:\n",
    "        del news_dic[words]\n",
    "\n",
    "\n",
    "temp = []\n",
    "for key in news_dic:\n",
    "    repeat = news_dic[key]\n",
    "    for i in range(repeat):\n",
    "        temp.append(key)\n",
    "\n",
    "for i in punctuation:\n",
    "    if i in temp:\n",
    "        print(i)\n",
    "\n",
    "news_token = temp\n",
    "\n",
    "\n",
    "news_vocab = {}\n",
    "for token in news_token:\n",
    "    if token in news_vocab:\n",
    "        news_vocab[token] += 1\n",
    "    else: \n",
    "        news_vocab[token] = 1\n",
    "\n",
    "\n",
    "\n",
    "print(f'Length of news tokens: {len(news_token)}')\n",
    "print(f'Length of news_vocab: {len(news_vocab)}')\n",
    "x = list(sorted(news_vocab.items(), key=lambda item: item[1]))\n",
    "print(f'Ordered: {x[-10:]}')\n",
    "\n",
    "print(len(news_vocab) / len(news_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of swbd tokens: 37929\n",
      "Length of swbd_vocab: 4722\n",
      "Ordered: [('laughter', 342), ('think', 419), ('well', 421), ('like', 451), (\"n't\", 784), ('uh-huh', 785), ('yeah', 1030), ('know', 1110), ('uh', 1265), (\"'s\", 1297)]\n",
      "0.12449576840939651\n"
     ]
    }
   ],
   "source": [
    "# Make all lower case\n",
    "swbd_token_lower_map = map(lambda x:x.lower(), swbd_tokens)\n",
    "swbd_token = list(swbd_token_lower_map)\n",
    "\n",
    "\n",
    "swbd_dic = {}\n",
    "for token in swbd_token:\n",
    "    if token not in swbd_dic:\n",
    "        swbd_dic[token] = 1\n",
    "    else:\n",
    "        swbd_dic[token] += 1\n",
    "\n",
    "for symbol in punctuation:\n",
    "    if symbol in swbd_token:\n",
    "        del swbd_dic[symbol]\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "# Remove stop words\n",
    "for words in stopWords:\n",
    "    if words in swbd_token:\n",
    "        del swbd_dic[words]\n",
    "\n",
    "temp = []\n",
    "for key in swbd_dic:\n",
    "    repeat = swbd_dic[key]\n",
    "    for i in range(repeat):\n",
    "        temp.append(key)\n",
    "\n",
    "for i in punctuation:\n",
    "    if i in temp:\n",
    "        print(i)\n",
    "\n",
    "swbd_token = temp\n",
    "\n",
    "\n",
    "swbd_vocab = {}\n",
    "for token in swbd_token:\n",
    "    if token in swbd_vocab:\n",
    "        swbd_vocab[token] += 1\n",
    "    else: \n",
    "        swbd_vocab[token] = 1\n",
    "\n",
    "\n",
    "\n",
    "print(f'Length of swbd tokens: {len(swbd_token)}')\n",
    "print(f'Length of swbd_vocab: {len(swbd_vocab)}')\n",
    "x = list(sorted(swbd_vocab.items(), key=lambda item: item[1]))\n",
    "print(f'Ordered: {x[-10:]}')\n",
    "\n",
    "print(len(swbd_vocab) / len(swbd_token))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"Playing games has always been thought to be important to \"\n",
    "    \"the development of well-balanced and creative children; \"\n",
    "    \"however, what part, if any, they should play in the lives \"\n",
    "    \"of adults has never been researched that deeply. I believe \"\n",
    "    \"that playing games is every bit as important for adults \"\n",
    "    \"as for children. Not only is taking time out to play games \"\n",
    "    \"with our children and other adults valuable to building \"\n",
    "    \"interpersonal relationships but is also a wonderful way \"\n",
    "    \"to release built up tension.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_token_10k = word_tokenize(shakespeare_data)[0:10000]\n",
    "news_token_10k = word_tokenize(news_data)[0:10000]\n",
    "swbd_token_10k = word_tokenize(swbd_data)[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46757\n"
     ]
    }
   ],
   "source": [
    "shakespeare_string = \"\"\n",
    "\n",
    "for item in shakespeare_token_10k:\n",
    "    shakespeare_string += item + \" \"\n",
    "\n",
    "print(len(shakespeare_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57163\n"
     ]
    }
   ],
   "source": [
    "news_string = \"\"\n",
    "\n",
    "for item in news_token_10k:\n",
    "    news_string += item + \" \"\n",
    "\n",
    "print(len(news_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41591\n"
     ]
    }
   ],
   "source": [
    "swbd_string = \"\"\n",
    "\n",
    "for item in swbd_token_10k:\n",
    "    swbd_string += item + \" \"\n",
    "\n",
    "print(len(swbd_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.02\n",
      "58.21\n",
      "89.68\n"
     ]
    }
   ],
   "source": [
    "print(textstat.flesch_reading_ease(shakespeare_string))\n",
    "print(textstat.flesch_reading_ease(news_string))\n",
    "print(textstat.flesch_reading_ease(swbd_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5\n",
      "13.6\n",
      "5.7\n"
     ]
    }
   ],
   "source": [
    "print(textstat.automated_readability_index(shakespeare_string))\n",
    "print(textstat.automated_readability_index(news_string))\n",
    "print(textstat.automated_readability_index(swbd_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.0\n",
      "13.8\n",
      "7.833333333333334\n"
     ]
    }
   ],
   "source": [
    "print(textstat.linsear_write_formula(shakespeare_string))\n",
    "print(textstat.linsear_write_formula(news_string))\n",
    "print(textstat.linsear_write_formula(swbd_string))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
